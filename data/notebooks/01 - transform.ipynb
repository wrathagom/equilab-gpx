{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abd31f9d-8035-47ba-9385-2c7dcd06ff3b",
   "metadata": {},
   "source": [
    "# Transform\n",
    "\n",
    "In this notebook we're going to Transform GPX files stored in Digital Ocean spaces.  If you notice, this project is missing an extract step. That's because I have a process setup to upload Equilab data using a Shotrcut on my phone, which is essentially my extract process. Equilab doens't have an API, so I can't extract the data in the traditional sense.\n",
    "\n",
    "Before we get to the fun stuff first we must import:\n",
    "\n",
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6d735432-4309-4951-82f0-6717aa167671",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import bucketstore\n",
    "from geopy import distance as geodistance\n",
    "import geopy\n",
    "import gpxpy\n",
    "import yaml\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50d06f4-cfbc-46f2-9f64-bb63aea9825f",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "With importing out of the way we'll get a few basics out of the way. The below cell:\n",
    " 1) Loads a file containing connection info to my DO Space\n",
    " 2) Uses the connection info to connect to the space\n",
    " 3) Loads the bucket that all of the data is stored in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "af9a2801-726f-46ba-8c68-b4417b733108",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"secrets.yml\", 'r') as ymlfile:\n",
    "    cfg = yaml.safe_load(ymlfile)\n",
    "\n",
    "bucketstore.login(\n",
    "    access_key_id=cfg['spaces']['access'],\n",
    "    secret_access_key=cfg['spaces']['secret'],\n",
    "    region='nyc3',\n",
    "    endpoint_url=cfg['spaces']['url']\n",
    ")\n",
    "bucket = bucketstore.get('wrathalake')\n",
    "\n",
    "ridesKey = 'intermediate/rides.json'\n",
    "processed = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3402a78f-bbf3-4a9f-85cf-037545c6817b",
   "metadata": {},
   "source": [
    "## Functions\n",
    "\n",
    "To keep the code clean we're going to encapsulate a few things in functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4874091a-2214-4c2b-88f3-6aa7bf192a40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def processFile(key):\n",
    "    gpx = gpxpy.parse(bucket[key])\n",
    "    \n",
    "    # Choosing to store coordinates in a dict to remove ambiguity. Apparently we can't all agree to use long, lat or lat, long\n",
    "    data = [{\n",
    "        'time': point.time,\n",
    "        'coords': {\n",
    "            'long': point.longitude,\n",
    "            'lat': point.latitude\n",
    "        },\n",
    "        'elevation': point.elevation * 3.28084\n",
    "    } for point in gpx.tracks[0].segments[0].points]\n",
    "    \n",
    "    # Calculating some ride metrics\n",
    "    totalDistance = 0\n",
    "    totalTime = (data[-1]['time'] - data[0]['time']).total_seconds()\n",
    "    totalClimb = 0\n",
    "    \n",
    "    prv = nxt = None\n",
    "    l = len(data)\n",
    "    \n",
    "    for index, obj in enumerate(data):\n",
    "        obj['index'] = index\n",
    "        \n",
    "        if index > 0:\n",
    "            prv = data[index - 1]\n",
    "\n",
    "        if index < (l - 1):\n",
    "            nxt = data[index + 1]\n",
    "\n",
    "        if prv is not None:\n",
    "            timeDelta = (obj['time'] - prv['time']).total_seconds()\n",
    "            \n",
    "            # geopy uses coordinates in the (lat, long) format, so we'll create that below.\n",
    "            distance = geodistance.geodesic((obj['coords']['lat'], obj['coords']['long']), (prv['coords']['lat'], prv['coords']['long'])).ft\n",
    "            totalDistance += distance\n",
    "            \n",
    "            speed = (distance / timeDelta) * 0.681818182\n",
    "            \n",
    "            if obj['elevation'] > prv['elevation']:\n",
    "                climb = obj['elevation'] - prv['elevation']\n",
    "                totalClimb += climb\n",
    "            else:\n",
    "                climb = None\n",
    "\n",
    "            obj['timeDelta'] = timeDelta\n",
    "            obj['distance'] = distance\n",
    "            obj['speed'] = speed\n",
    "            obj['climb'] = climb\n",
    "            obj['drop'] = (prv['elevation'] - obj['elevation']) if obj['elevation'] < prv['elevation'] else None\n",
    "            \n",
    "        \n",
    "    rideData = {\n",
    "        'rawFile': key,\n",
    "        'interFile': key.replace('raw', 'intermediate').replace('gpx', 'json'),\n",
    "        'rideDate': data[0]['time'].strftime(\"%Y%m%d\"),\n",
    "        'totalTime': totalTime,\n",
    "        'totalDistance': totalDistance,\n",
    "        'totalClimb': totalClimb,\n",
    "        'averageSpeed': (totalDistance / totalTime) * 0.681818182,\n",
    "        'inElastic': False\n",
    "    }\n",
    "    \n",
    "    # Once we're done processing we need to set the time to be something that can be stored in JSON\n",
    "    for index, obj in enumerate(data):\n",
    "        timestamp = obj['time'].isoformat()\n",
    "        obj['time'] = timestamp\n",
    "            \n",
    "    return data, rideData\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3da1b6-73fd-466b-beb5-c0ec6aa45fee",
   "metadata": {},
   "source": [
    "## History\n",
    "\n",
    "When ever this process runs it uses a Rides file that documents each ride. If a ride is listed in this file it means that it has already been processed and thus doesn't need processed again.\n",
    "\n",
    "So first things first, let's load the rides file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2f99d812-b85c-4c55-9b7c-52e88947163f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 9 rides that have been processed.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    rides = json.loads(bucket['intermediate/rides.json'])\n",
    "except:\n",
    "    bucket[ridesKey] = json.dumps({})\n",
    "    rides = {}\n",
    "\n",
    "rideKeys = rides.keys()\n",
    "\n",
    "print('There are {} rides that have been processed.'.format(len(rides.keys())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae5bb8e-e414-4555-a9cb-848ca2dbd738",
   "metadata": {},
   "source": [
    "## Raw Ride Files\n",
    "\n",
    "Now that we have a list of files that have been processed we can check all of the files against it and process just the ones that need it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "42502f04-d4d0-4f99-a381-1f30a5ea0d14",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 12 rides total.\n"
     ]
    }
   ],
   "source": [
    "objects = bucket.list(prefix='raw/sources/equilab/')\n",
    "\n",
    "print('There are {} rides total.'.format(len(objects)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a4f28c12-8ae6-42f6-aeb6-6307dc62a489",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing raw/sources/equilab/training-2023-08-12.gpx\n",
      "Processing raw/sources/equilab/training-2023-08-13.gpx\n"
     ]
    }
   ],
   "source": [
    "for obj in objects:\n",
    "    if obj.endswith('.gpx') and obj not in rideKeys:\n",
    "        print('Processing {}'.format(obj))\n",
    "        fileData, rideData = processFile(obj)\n",
    "        \n",
    "        newKey = obj.replace('raw', 'intermediate').replace('gpx', 'json')\n",
    "        bucket[newKey] = json.dumps(fileData)\n",
    "        \n",
    "        rides[obj] = rideData\n",
    "        processed = True\n",
    "\n",
    "# If we've processed any data, then we need to save the rides data back to the bucket.\n",
    "if processed:\n",
    "    bucket[ridesKey] = json.dumps(rides)\n",
    "else:\n",
    "    print('Looks like no data was processed')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b1dd8a-9fdc-43b2-990c-fd1c8bfb25c7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Helper Functions\n",
    "\n",
    "The below lines of code are meant as helper functions and in general they should be left commented out. But it's not always a bad thing to leave them uncommented and save them as history for the scheduled process that runs the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "68fc7d13-70a5-46eb-8cdd-abc109fbf475",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# View the rides JSON file\n",
    "# bucket[ridesKey]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8146a328-a3f0-4397-8100-dfbb89ed59ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Delete the rides file, which essentially will restart processing\n",
    "# del bucket[ridesKey]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "97bcf0f4-b791-461c-9d1c-7468221083c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# List all of the ride data in the intermediate directory\n",
    "# bucket.list(prefix='intermediate/sources/equilab/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "69902d61-bd45-4f82-905c-9398436d7076",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Delete all of the processes rides\n",
    "# for key in bucket.list(prefix='intermediate/sources/equilab/'):\n",
    "#     del bucket[key]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "810eea82-6abf-4bdf-b441-202ffcc50bca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'time': '2023-08-13T20:34:08+00:00',\n",
       "  'coords': {'long': -85.3560152, 'lat': 38.2852281},\n",
       "  'elevation': 833.005276,\n",
       "  'index': 0},\n",
       " {'time': '2023-08-13T20:34:09+00:00',\n",
       "  'coords': {'long': -85.3560147, 'lat': 38.2852272},\n",
       "  'elevation': 833.989528,\n",
       "  'index': 1,\n",
       "  'timeDelta': 1.0,\n",
       "  'distance': 0.35780751633480307,\n",
       "  'speed': 0.24395967029333074,\n",
       "  'climb': 0.9842519999999695,\n",
       "  'drop': None},\n",
       " {'time': '2023-08-13T20:34:10+00:00',\n",
       "  'coords': {'long': -85.3560108, 'lat': 38.2852248},\n",
       "  'elevation': 833.33336,\n",
       "  'index': 2,\n",
       "  'timeDelta': 1.0,\n",
       "  'distance': 1.4202644144790146,\n",
       "  'speed': 0.9683621010393763,\n",
       "  'climb': None,\n",
       "  'drop': 0.6561679999999797}]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View a few lines from the last fileData processed\n",
    "if processed:\n",
    "    fileData[:3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "988eedae-47fb-414f-a7e3-5129c0a41aa6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['raw/sources/equilab/training-2023-11-05.gpx',\n",
       " 'raw/sources/equilab/training-2023-11-18.gpx',\n",
       " 'raw/sources/equilab/training-2023-11-19.gpx',\n",
       " 'raw/sources/equilab/training-2023-08-12.gpx',\n",
       " 'raw/sources/equilab/training-2023-08-13.gpx']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print all of the rides that have been processed\n",
    "if processed:\n",
    "    list(rides.keys())[-5:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "681602b7-275a-4b95-8346-13b8ca5d49e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rawFile': 'raw/sources/equilab/training-2023-08-13.gpx',\n",
       " 'interFile': 'intermediate/sources/equilab/training-2023-08-13.json',\n",
       " 'rideDate': '20230813',\n",
       " 'totalTime': 6250.003,\n",
       " 'totalDistance': 29337.12579091715,\n",
       " 'totalClimb': 6869.750875999966,\n",
       " 'averageSpeed': 3.200412187301101,\n",
       " 'inElastic': False}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the ride details for one of the rides above, by default it shows the lastest\n",
    "if processed:\n",
    "    rides[list(rides.keys())[-1]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d33431d-6373-452f-8c6f-74f9bdc38381",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
